{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "hbdQ30o4ENTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filename : update.py\n",
        "\n",
        "Libraries Required"
      ],
      "metadata": {
        "id": "TE7TWhY6_1HP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NAj8ul0m9PuB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import schedule\n",
        "import requests\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert the how much to cut csv for feature extraction and dataset threshold, as giving lot of URL crashes the code and 5000 is the limit of dataset."
      ],
      "metadata": {
        "id": "fOQa5LmN_75u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_ROWS_REPO = 8000\n",
        "MAX_ROWS_DATASET = 5000"
      ],
      "metadata": {
        "id": "JinA-Ga29UNe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the online-valid.csv basically phishing repo with over 60000 phishing URLs and then trimming it to 8000 URLs for feature extraction."
      ],
      "metadata": {
        "id": "aAkHwh-hANtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_phishtank_repo():\n",
        "    url = \"http://data.phishtank.com/data/online-valid.csv\"\n",
        "    response = requests.get(url)\n",
        "    with open('online-valid.csv', 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(\"Phishtank repo downloaded.\")\n",
        "\n",
        "    # Load the phishrepo\n",
        "    df_phishrepo = pd.read_csv('online-valid.csv')\n",
        "    # Trim the data to the first 8000 rows\n",
        "    df_phishrepo = df_phishrepo.head(MAX_ROWS_REPO)\n",
        "    # Save the trimmed data\n",
        "    df_phishrepo.to_csv('online-valid.csv', index=False)\n",
        "    print(\"Phishtank repo trimmed.\")"
      ],
      "metadata": {
        "id": "BnxlLhxr9VXF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After trimming the feature_extract.py code will be executed on the csv file to extract out dataset and then further dataset will be trimmed to 5000 Rows."
      ],
      "metadata": {
        "id": "aolh0ASlAdUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scripts():\n",
        "    try:\n",
        "        print(\"Running scripts...\")\n",
        "        subprocess.run([\"python\", \"features_extract.py\"])\n",
        "        # Load the features dataset\n",
        "        df_features = pd.read_csv('dataset_output.csv')\n",
        "        # Trim the data to the first 5000 rows\n",
        "        df_features = df_features.head(MAX_ROWS_DATASET)\n",
        "        # Save the trimmed data\n",
        "        df_features.to_csv('dataset_output.csv', index=False)\n",
        "        print(\"Features dataset trimmed.\")\n",
        "        subprocess.run([\"python\", \"rfcmodel.py\"])\n",
        "        print(\"Scripts run successfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred while running the scripts:\", e)"
      ],
      "metadata": {
        "id": "5hWhnss79WEk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally setting up the update to run everyday at midnight 12:00 am to keep the phishing dataset updated."
      ],
      "metadata": {
        "id": "0205cxAdArBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def job():\n",
        "    print(\"Job Started at: \", datetime.now())\n",
        "    download_phishtank_repo()\n",
        "    run_scripts()\n",
        "    print(\"Job Finished at: \", datetime.now())\n",
        "\n",
        "schedule.every().day.at(\"00:00\").do(job)\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "t4JN9vfT9YRn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}