{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-whois\n",
        "!pip install tldextract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYek25v3--Ik",
        "outputId": "19cc95b9-82c4-465c-99d9-6055b6b7b266"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-whois in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from python-whois) (0.18.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.27.1)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file>=1.4->tldextract) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "File name : features_extract.py\n",
        "\n",
        "These are all the library used for feature extraction code."
      ],
      "metadata": {
        "id": "yj9r9ZmKszw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import whois\n",
        "import socket\n",
        "import re\n",
        "from googlesearch import search\n",
        "from urllib.parse import urlparse\n",
        "import tldextract\n",
        "import csv\n",
        "import socket\n",
        "import urllib.parse\n",
        "import string\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil.parser import parse as date_parse\n",
        "from requests.exceptions import RequestException, TooManyRedirects\n",
        "from ssl import SSLContext, PROTOCOL_TLSv1_2, CERT_NONE, PROTOCOL_TLS\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "cZ8BpoCxscHH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majestic million doesnt provide URLs with the 'www' prefix so I added this function."
      ],
      "metadata": {
        "id": "_MiEbMbgtCYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(url, headers):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5, headers=headers)\n",
        "        return response\n",
        "    except:\n",
        "        if not url.startswith('www.'):\n",
        "            url = 'www.' + url\n",
        "        try:\n",
        "            response = requests.get(url, timeout=5, headers=headers)\n",
        "            return response\n",
        "        except:\n",
        "            return None"
      ],
      "metadata": {
        "id": "OTeSPtR9s5y-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22 Functions for feature extraction."
      ],
      "metadata": {
        "id": "HlWZaerktXB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using IP address\n",
        "def get_using_ip(url):\n",
        "    try:\n",
        "        parsed_url = urlparse(url)\n",
        "        domain = parsed_url.netloc.split(':')[0]\n",
        "        ip = socket.gethostbyname(domain)\n",
        "        return 1 if ip in url else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Using IP Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Long URL\n",
        "def get_long_url(url):\n",
        "    try:\n",
        "        url_length = len(url)\n",
        "        return 1 if url_length > 54 else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Long Url Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Short URL\n",
        "def get_short_url(url):\n",
        "    try:\n",
        "        match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
        "                        'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
        "                        'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
        "                        'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
        "                        'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
        "                        'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
        "                        'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net', url)\n",
        "        if match:\n",
        "            return 1\n",
        "        return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Short URL Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "# Symbol\n",
        "def get_symbol(url):\n",
        "    try:\n",
        "        return 1 if \"@\" in url else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Get Symbol URL Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Redirecting\n",
        "def get_redirecting(url):\n",
        "    session = requests.Session()\n",
        "    session.max_redirects = 5\n",
        "    redirect_count = 0\n",
        "\n",
        "    try:\n",
        "        response = session.get(url, timeout=5, allow_redirects=False)\n",
        "        while response.status_code in (301, 302):\n",
        "            redirect_count += 1\n",
        "            if redirect_count > session.max_redirects:\n",
        "                break\n",
        "            redirect_url = response.headers['Location']\n",
        "            response = session.get(redirect_url, timeout=5, allow_redirects=False)\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"Redirecting Timeout occurred for URL: {url}\")\n",
        "        return -1\n",
        "    except requests.exceptions.TooManyRedirects:\n",
        "        print(f\"Redirecting Too many redirects for URL: {url}\")\n",
        "        return -1\n",
        "    except Exception as e:\n",
        "        print(f\"Redirecting Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "    return redirect_count\n",
        "\n",
        "# Prefix-Suffix\n",
        "def get_prefix_suffix(url):\n",
        "    try:\n",
        "        parsed_url = urlparse(url)\n",
        "        domain_name = parsed_url.netloc\n",
        "        return 1 if '-' in domain_name else 0\n",
        "    except Exception as e:\n",
        "        print(f\"Prefix Suffix Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Subdomains\n",
        "def get_subdomains(url):\n",
        "    try:\n",
        "        ext = tldextract.extract(url)\n",
        "        subdomain_count = ext.subdomain.count('.')\n",
        "        return subdomain_count\n",
        "    except Exception as e:\n",
        "        print(f\"Subdomains Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# HTTPS\n",
        "def get_https(url):\n",
        "    try:\n",
        "        return 1 if url.startswith(\"https\") else 0\n",
        "    except Exception as e:\n",
        "        print(f\"HTTPS Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# SSL\n",
        "def has_ssl(url):\n",
        "    try:\n",
        "\n",
        "        if not url.startswith('http'):\n",
        "            url = 'https://' + url\n",
        "\n",
        "        response = requests.get(url, timeout=5)\n",
        "\n",
        "        if response.url.startswith('https://'):\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(f\"SSL Error processing URL {url}: Connection Error\")\n",
        "        return -1\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(f\"SSL Timeout occurred for URL: {url}\")\n",
        "        return -1\n",
        "    except requests.exceptions.TooManyRedirects:\n",
        "        print(f\"SSL Too many redirects for URL: {url}\")\n",
        "        return -1\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"SSL General Request Exception for URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Domain Registration Length\n",
        "def get_domain_reg_length(url):\n",
        "    try:\n",
        "        domain_info = whois.whois(url)\n",
        "        if domain_info.creation_date is None:\n",
        "            return -1\n",
        "\n",
        "        if isinstance(domain_info.creation_date, list):\n",
        "            creation_date = domain_info.creation_date[0]\n",
        "        else:\n",
        "            creation_date = domain_info.creation_date\n",
        "\n",
        "        if isinstance(creation_date, str):\n",
        "            creation_date = date_parse(creation_date)\n",
        "\n",
        "        age_in_days = (datetime.now() - creation_date).days\n",
        "\n",
        "        # Check if domain is less than 1 year old\n",
        "        if age_in_days < 365:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Domain Reg Length Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "\n",
        "# Favicon\n",
        "def get_favicon(url, content):\n",
        "    try:\n",
        "        soup = BeautifulSoup(content, \"html.parser\")\n",
        "        favicon = soup.find(\"link\", rel=\"shortcut icon\")\n",
        "        if favicon:\n",
        "            favicon_href = favicon.get(\"href\")\n",
        "            favicon_url = urlparse(favicon_href)\n",
        "            if favicon_url.netloc == \"\":\n",
        "                return 0\n",
        "        return 1\n",
        "    except Exception as e:\n",
        "        print(f\" Favicon Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Non-standard port\n",
        "def get_non_std_port(url):\n",
        "    try:\n",
        "        parsed_url = urlparse(url)\n",
        "        port = parsed_url.port\n",
        "        if port is None:\n",
        "            return 0\n",
        "        elif port not in [80, 443]:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except Exception as e:\n",
        "        print(f\" Non-standard Error Port processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Dots\n",
        "def count_dots(url):\n",
        "    try:\n",
        "        return url.count('.')\n",
        "    except Exception as e:\n",
        "        print(f\" Count dots Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "# Rediretion '//'\n",
        "def count_double_slash(url):\n",
        "    try:\n",
        "        return url[url.find('//')+2:].count('//')\n",
        "    except Exception as e:\n",
        "        print(f\" Double Slash Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "\n",
        "def email_in_url(url):\n",
        "    try:\n",
        "        return 1 if re.search(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", url) else 0\n",
        "    except Exception as e:\n",
        "        print(f\" Email in Url Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "def abnormalURL(url):\n",
        "    try:\n",
        "\n",
        "        parsed_url = urllib.parse.urlparse(url)\n",
        "        url_path_and_params = parsed_url.path + parsed_url.params + parsed_url.query + parsed_url.fragment\n",
        "\n",
        "        # Check if more than 50% of characters are non-alphanumeric\n",
        "        alphanumeric_chars = string.ascii_letters + string.digits\n",
        "        non_alphanumeric_chars = [ch for ch in url_path_and_params if ch not in alphanumeric_chars]\n",
        "        if len(non_alphanumeric_chars) > 0.5 * len(url_path_and_params):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except Exception as e:\n",
        "        print(f\" Abnormal URL Error processing URL {url}: {str(e)}\")\n",
        "        return -1\n",
        "\n",
        "def WebsiteForwarding(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.history:\n",
        "            return 1  # there is a redirect, hence website forwarding is enabled\n",
        "        else:\n",
        "            return 0  # no redirect, hence no website forwarding\n",
        "    except Exception as e:\n",
        "        print(f\" Website Forwarding Error processing URL {url}: {str(e)}\")\n",
        "        return -1\n",
        "\n",
        "def DisableRightClick(url):\n",
        "    try:\n",
        "        # Get webpage content\n",
        "        response = requests.get(url)\n",
        "        content = response.text\n",
        "\n",
        "        # Look for common patterns that disable right-click\n",
        "        patterns = [\n",
        "            r\"contextmenu[^{]*return false\",\n",
        "            r\"event.button ?== ?2[^{]*return false\",\n",
        "            r\"addEventListener\\(['\\\"]contextmenu['\\\"]\",\n",
        "            r\"oncontextmenu\\s*=\\s*['\\\"]return false['\\\"]\"\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            if re.search(pattern, content, re.IGNORECASE):\n",
        "                # Return '1' if pattern that disables right-click is found\n",
        "                return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Disable Right Click Error processing URL {url}: {str(e)}\")\n",
        "        return -1\n",
        "\n",
        "def UsingPopupWindow(url):\n",
        "    try:\n",
        "        # Get webpage content\n",
        "        response = requests.get(url)\n",
        "        content = response.text\n",
        "\n",
        "        # Look for \"window.open\" pattern\n",
        "        if re.search(r\"window.open\", content, re.IGNORECASE):\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    except Exception as e:\n",
        "        print(f\" Popup Error processing URL {url}: {str(e)}\")\n",
        "        return -1\n",
        "\n",
        "def age_domain(url):\n",
        "    try:\n",
        "        w = whois.whois(url)\n",
        "        if w.creation_date is not None:\n",
        "            if type(w.creation_date) is list:\n",
        "                creation_date = w.creation_date[0]\n",
        "            else:\n",
        "                creation_date = w.creation_date\n",
        "            ageofdomain = abs((creation_date - datetime.now()).days)\n",
        "            if ageofdomain <= 60:\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "        else:\n",
        "            return -1\n",
        "    except Exception as e:\n",
        "        print(f\"Age Domain Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "def DNSRecording(url):\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        if domain:\n",
        "            record = socket.gethostbyname(domain)\n",
        "            if record:\n",
        "                return 0  # DNS record exists, not bad\n",
        "            else:\n",
        "                return 1  # No DNS record, bad\n",
        "        else:\n",
        "            return 1  # No domain in URL, bad\n",
        "    except Exception as e:\n",
        "        print(f\" DNS Recording Error processing URL {url}: {e}\")\n",
        "        return -1\n",
        "\n",
        "def LinksPointingToPage(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        for link in links:\n",
        "            if 'http' in link['href'] and url not in link['href']:\n",
        "                return 1  # External link found\n",
        "        return 0  # No external links\n",
        "    except Exception as e:\n",
        "        print(f\"Links Pointing Error processing URL {url}: {e}\")\n",
        "        return -1"
      ],
      "metadata": {
        "id": "0VJg7ZFCtXFC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing of URLs"
      ],
      "metadata": {
        "id": "6k-NUYAFuYBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_url(url_label):\n",
        "    url, label = url_label\n",
        "    headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "    # Check if the URL starts with a scheme\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        # Prepend 'http://' if no scheme is found\n",
        "        url = 'https://' + url\n",
        "\n",
        "    try:\n",
        "        # Add timeout to request.get() call\n",
        "        response = requests.get(url, timeout=5, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            features = extract_features(url, label)\n",
        "            if features is not None:\n",
        "                features[\"Url\"] = url\n",
        "            return features\n",
        "        else:\n",
        "            print(f\"Process URL not accessible: {url}\")\n",
        "            return None\n",
        "    except requests.exceptions.ReadTimeout as e:\n",
        "        print(f\"Process URL Timeout error processing URL {url}: {e}\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Process URL Error processing URL {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Process URL Unexpected error processing URL {url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "9YtV6v8OuYFx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arranging the features for dataset"
      ],
      "metadata": {
        "id": "m6xBypBduEYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction\n",
        "def extract_features(url,label=None):\n",
        "    features = {}\n",
        "    try:\n",
        "        parsed_url = urlparse(url)\n",
        "        if not parsed_url.scheme:\n",
        "            url = 'http://' + url\n",
        "        response = requests.get(url, timeout=10)\n",
        "        content = response.content\n",
        "\n",
        "        features['UsingIp'] = get_using_ip(url)\n",
        "        features['longUrl'] = get_long_url(url)\n",
        "        features['shortUrl'] = get_short_url(url)\n",
        "        features['symbol'] = get_symbol(url)\n",
        "        features['redirecting'] = get_redirecting(url)\n",
        "        features['prefixSuffix'] = get_prefix_suffix(url)\n",
        "        features['SubDomains'] = get_subdomains(url)\n",
        "        features['Https'] = get_https(url)\n",
        "        features['hasSsl'] = has_ssl(url)\n",
        "        features['DomainRegLen'] = get_domain_reg_length(url)\n",
        "        features['Favicon'] = get_favicon(url, content)\n",
        "        features['NonStdPort'] = get_non_std_port(url)\n",
        "        features['label'] = label\n",
        "        features['Dots'] = count_dots(url)\n",
        "        features['Redirection //'] = count_double_slash(url)\n",
        "        features['InfoEmail'] = email_in_url(url)\n",
        "        features['AbnormalURL'] = abnormalURL(url)\n",
        "        features['WebsiteForwarding'] = WebsiteForwarding(url)\n",
        "        features['DisableRightClick'] = DisableRightClick(url)\n",
        "        features['UsingPopupWindow'] = UsingPopupWindow(url)\n",
        "        features['AgeofDomain'] = age_domain(url)\n",
        "        features['DNSRecording'] = DNSRecording(url)\n",
        "        features['LinksPointingToPage'] = LinksPointingToPage(url)\n",
        "\n",
        "        if label is not None:\n",
        "            features['label'] = label\n",
        "\n",
        "    except (RequestException, TooManyRedirects) as e:\n",
        "        print(f\"Feature Extraction Too many Redirects Error processing URL {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Feature Extraction Error processing URL {url}: {e}\")\n",
        "        return None\n",
        "    return features"
      ],
      "metadata": {
        "id": "7zN79pZruFD5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function where I set the names of the input and output files."
      ],
      "metadata": {
        "id": "mnjCEPfw0dT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define your datasets\n",
        "    datasets = [\n",
        "        {\n",
        "            \"input_file\": \"legitrepo.csv\",\n",
        "            \"dataset_type\": \"0\",\n",
        "            \"output_file\": \"legit_output.csv\"\n",
        "        },\n",
        "        {\n",
        "            \"input_file\": \"phishrepocsv\",\n",
        "            \"dataset_type\": \"1\",\n",
        "            \"output_file\": \"phishing_output.csv\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    num_threads = 100\n",
        "\n",
        "    # Start a new thread for each dataset\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        executor.map(process_dataset, datasets)\n"
      ],
      "metadata": {
        "id": "8gDa4wtE0ddS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of Dataset"
      ],
      "metadata": {
        "id": "ar-eFWC7y2VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset):\n",
        "    input_file = dataset[\"input_file\"]\n",
        "    dataset_type = dataset[\"dataset_type\"]\n",
        "    output_file = dataset[\"output_file\"]\n",
        "    num_threads = 100\n",
        "\n",
        "    with open(input_file, \"r\", encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        urls_labels = [(row['url'], dataset_type) for row in reader]\n",
        "\n",
        "    with open(output_file, \"w\", newline=\"\", encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\n",
        "            \"UsingIp\",\n",
        "            \"longUrl\",\n",
        "            \"shortUrl\",\n",
        "            \"symbol\",\n",
        "            \"redirecting\",\n",
        "            \"prefixSuffix\",\n",
        "            \"SubDomains\",\n",
        "            \"Https\",\n",
        "            \"hasSsl\",\n",
        "            \"DomainRegLen\",\n",
        "            \"Favicon\",\n",
        "            \"NonStdPort\",\n",
        "            \"Dots\",\n",
        "            \"Redirection //\",\n",
        "            \"InfoEmail\",\n",
        "            \"AbnormalURL\",\n",
        "            \"WebsiteForwarding\",\n",
        "            \"DisableRightClick\",\n",
        "            \"UsingPopupWindow\",\n",
        "            \"AgeofDomain\",\n",
        "            \"DNSRecording\",\n",
        "            \"LinksPointingToPage\",\n",
        "            \"label\",\n",
        "            \"Url\"\n",
        "        ]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            for result in executor.map(process_url, urls_labels):\n",
        "                if result:\n",
        "                    writer.writerow(result)"
      ],
      "metadata": {
        "id": "L3Q2x2zCy3_0"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}